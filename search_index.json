[
["index.html", "STAT 5361: Statistical Computing, Fall 2018 Chapter 1 Prerequisites 1.1 A Teaser Example: Likelihood Estimation 1.2 Computer Arithmetics 1.3 Exercises 1.4 Course Project", " STAT 5361: Statistical Computing, Fall 2018 Jun Yan 2018-09-21 Chapter 1 Prerequisites We assume that students can use R/Rstudio comfortably. If you are new to R, an extra amount of hard work should be devoted to make it up. In addition to the official documentations from the R-project, the presentations at a [local SIAM workshop in Spring 2018] (https://siam.math.uconn.edu/events/) given by Wenjie Wang, a PhD student in Statistics at the time, can be enlighting: - https://github.com/wenjie2wang/2018-01-19-siam - https://github.com/wenjie2wang/2018-04-06-siam If you have used R, but never paid attention to R programming styles, a style clinic would be a necessary step. A good place to start is Google’s R style guide at https://google.github.io/styleguide/Rguide.xml. From my experience, the two most commonly overlooked styles for beginners are spacing and indentation. Appropriate spacing and indentation would immediately make crowdly piled code much more eye-friendly. Such styles can be automatically enforced by R packages such as formatr or lintr. Tow important styles that cannot be automatically corrected are naming and documentation. As in any programming languate, naming of R objects (variables, functions, files, etc.) shoud be informative, concise, and consistent with certain naming convention. Documentation needs to be sufficient, concise, and kept close to the code; tools like R package roxygen2 can be very helpful. See Hadley Wichham’s online book http://style.tidyverse.org/ for more detailed tips. For intermediate R users who want a skill lift, The Advanced R Programming book (Wickham 2014) by Hadley Wickham is available at https://adv-r.hadley.nz/. The source that generated the book is kindly made available at GitHub: https://github.com/hadley/adv-r. It is a great learning experience to complile the book from the source, during which you may pick up many necessary skills. The homework, exam, and project will be completed by R Markdown. Following the step by step the instructions in Yihui Xie’s online book on bookdown (Xie 2016).at https://bookdown.org/yihui/bookdown/, you will be amazed how quickly you can learn to produce cool-looking documents and even book manuscripts. If you are a keener, you may as well following Yihui’s blogdown, see online book https://bookdown.org/yihui/blogdown/, to build your own website using R Markdown. All your source code will be version controlled by git and archived on GitHub. RStudio has made using git quite straightforward. The online tutorial by Jenny Bryan, Happy Git and GitHub for the useR, is a very useful tool to get started 1.1 A Teaser Example: Likelihood Estimation In mathematical statistics, we have learned that, under certain regularity conditions, the maximum likelihood estimator (MLE) is consistent, asymptotically normal, and most efficient. The asymptotic variance of the estimator if the inverse of the Fisher information matrix. Specifically, let \\(X_1, \\ldots, X_n\\) be a random sample from a distribution with density \\(f(x; \\theta)\\), where \\(\\theta\\) is a parameter. How do we obtain the MLE? set.seed(123) n &lt;- 100 x &lt;- rgamma(n, shape = 2, scale = 4) hist(x) Package MASS provides a function fitdistr() to obtain the MLE for univariate distributions with a random sample. We can learn two things from this function. First, an objective function representing the negative loglikelihood is formed, depending on the input of the density function, and fed to the optimizer function optim. Second, the variance estimator of the MLE is obtained by inverting the Hessian matrix of the objective function, which is an estimator of the Fisher information matrix. For commonly used distributions, starting values are not necessary. The function computes moment estimator and use them as starting values. MASS::fitdistr(x, densfun = &quot;gamma&quot;) ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## shape rate ## 2.19321713 0.31850407 ## (0.28968120) (0.04724651) For distributions not in the provided list, the densfun needs to be a function returning a density evaluated at its first arguments, in which case, the start argument needs to be a named list to feed to the optimizer. For example, pretend that someone has a fancy distribution which is just a rename of the gamma distribution. Its density function is defined based on the gamma density dgamma. dfancy &lt;- function(x, shape, scale, log = FALSE) { dgamma(x, shape = shape, scale = scale, log = log) } suppressWarnings(MASS::fitdistr(x, densfun = dfancy, start = list(shape = 10, scale = 20))) ## shape scale ## 2.1931194 3.1400457 ## (0.2897513) (0.4659625) The stats4 package provides MLE using S4 classes. nll &lt;- function(shape, scale) -sum(dfancy(x, shape, scale, TRUE)) suppressWarnings(fit &lt;- stats4::mle(nll, start = list(shape = 10, scale = 10))) stats4::summary(fit) ## Maximum likelihood estimation ## ## Call: ## stats4::mle(minuslogl = nll, start = list(shape = 10, scale = 10)) ## ## Coefficients: ## Estimate Std. Error ## shape 2.193213 0.2896847 ## scale 3.139686 0.4657464 ## ## -2 log L: 557.1586 1.2 Computer Arithmetics Burns (2012) summarizes many traps beginners may fall into. The first one is the floating number trap. Are you surprised by what you see in the following? ## floating point traps .1 == .3 / 3 ## [1] FALSE seq(0, 1, by = .1) == 0.3 ## [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE seq(0, 1, by = .1) ## [1] 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 unique(c(.3, .4 - .1, .5 - .2, .6 - .3, .7 - .4)) ## [1] 0.3 0.3 0.3 c(.3, .4 - .1, .5 - .2, .6 - .3, .7 - .4) ## [1] 0.3 0.3 0.3 0.3 0.3 options(digits = 20) .1 ## [1] 0.10000000000000000555 .3 / 3 ## [1] 0.099999999999999991673 How does a computer represent a number? Computers use switches, which has only two states, on and off, to represent numbers. That is, it uses the binary number system. With a finite (though large) number switches on a computer, not all real numbers are representable. Binary numbers are made of bits, each of which represent a switch. A byte is 8 bits, which represents one character. 1.2.1 Integers An integer on many computers has length 4-byte or 32-bit. It is represented by the coefficients \\(x_i\\) in \\[ \\sum_{i=1}^{32} x_i 2^{i-1} - 2^{31}, \\] where \\(x_i\\) is either 0 or 1. imax &lt;- .Machine$integer.max imax ## [1] 2147483647 imax + 1L ## Warning in imax + 1L: NAs produced by integer overflow ## [1] NA log(imax, base = 2) ## [1] 30.999999999328192501 u &lt;- 0L b &lt;- 1L for (i in 1L:31L) { u &lt;- u + b b &lt;- b * 2L } ## Warning in b * 2L: NAs produced by integer overflow u ## [1] 2147483647 1000L * 1000L * 1000L ## [1] 1000000000 1000L * 1000L * 1000L * 1000L ## Warning in 1000L * 1000L * 1000L * 1000L: NAs produced by integer overflow ## [1] NA 1000L * 1000L * 1000L * 1000 ## [1] 1e+12 1.2.2 Floating Point A floating point number is represented by \\[ (-1)^{x_0}\\left(1 + \\sum_{i=1}^t x_i 2^{-i}\\right) 2^{k} \\] for \\(x_i \\in \\{0, 1\\}\\), where \\(k\\) is an integer called the exponent, \\(x_0\\) is the sign bit. The fractional part \\(\\sum_{i=1}^t x_i 2^{-i}\\) is the significand. By convention (for unique representation), the exponent is chosen so that the fist digit of the significand is 1, unless it would result in the exponent being out of range. A standard double precision representation uses 8 bytes or 64 bits: a sign bit, an 11 bit exponent, and \\(t = 52\\) bits for the significand. With 11 bits there are \\(2^{11} = 2048\\) possible values for the exponent, which are usually shifted so that the allowed values range from \\(-1022\\) to 1023, again with one special sign value. The IEEE standard for floating point arithmetic calls for basic arithmetic operations to be performed to higher precision, and then rounded to the nearest representable floating point number. Arithmetic underflow occurs where the result of a calculation is a smaller number than what the computer can actually represent. Arithmetic overflow occurs when a calculation produces a result that is greater in magnitude than what the computer can represent. Here is a demonstration (Gray 2001). options(digits = 20) .Machine$double.eps ## [1] 2.2204460492503130808e-16 .Machine$double.base ^ .Machine$double.ulp.digits ## [1] 2.2204460492503130808e-16 1 + .Machine$double.eps == 1 ## [1] FALSE 1 + .Machine$double.base ^ (.Machine$double.ulp.digits - 1L) == 1 ## [1] TRUE ## largest possible significand u &lt;- 0 for (i in 1L:53) u &lt;- u + 2^(-i) u ## [1] 0.99999999999999988898 ## and the largest possible exponent--note that calculating 2^1024 ## directly overflows u * 2 * 2 ^ 1023 ## [1] 1.7976931348623157081e+308 ## next largest floating point number overflows (u * 2 * 2 ^ 1023) * (1 + 1/2^52) ## [1] Inf ## smallest possible mantissa and smallest possible exponent 1 / 2^52 * 2^(-1022) ## [1] 4.9406564584124654418e-324 1 / 2^52 * 2^(-1022) / 2 ## [1] 0 1.2.3 Error Analysis More generally, a floating point number with base \\(\\beta\\) can be represented as \\[ (-1)^{x_0}\\left(\\sum_{i=1}^t x_i \\beta^{-i}\\right) \\beta^{k}, \\] where \\(x_0 \\in \\{0, 1\\}\\), \\(x_i \\in \\{0, 1, \\ldots, \\beta - 1\\}\\), \\(i = 1, \\ldots, t\\), and \\(E_{\\min} \\le k \\le E_{\\max}\\) for some \\(E_{\\min}\\) and \\(E_{\\max}\\). For a binary system \\(\\beta = 2\\) while for the decimal system \\(\\beta = 10\\). To avoid multiple representations, a normalized representation is defined by forcing the first significant digit (\\(x_1\\)) to be non-zero. The numbers \\(\\{x: |x| &lt; \\beta^{E_{\\min} - 1}\\}\\) (called subnormals) can not be normalized and, are sometimes excluded from the system. If an operation results in a subnormal number underflow is said to have occurred. Similarly if an operation results in a number with the exponent \\(k &gt; E_{\\max}\\), overflow is said to have occurred. Let \\(f(x)\\) be the floating point representation of \\(x\\). The relative error of \\(f(x)\\) is \\(\\delta_x = [f(x) - x] / x\\). The basic formula for error analysis is \\[ f(x \\circ y) = (1+ \\epsilon_m) (x \\circ y) \\] where \\(\\circ\\) is an operator (e.g., add, subtract, multiple, divide, etc.), \\((x \\circ y)\\) is the exact result, and \\(\\epsilon_m\\) is the smallest positive floating-point number \\(z\\) such that \\(1 + z \\ne 1\\). Add demonstration from Section 1.3 of Gray (2001); see also [notes] (https://people.eecs.berkeley.edu/~demmel/cs267/lecture21/lecture21.html). (Section 1.4 from Gray) Consider computing \\(x + y\\), where \\(x\\) and \\(y\\) have the same sign. Let \\(\\delta_x\\) and \\(\\delta_y\\) be the relative error in the floating point representation of \\(x\\) and \\(y\\), respectively (e.g., \\(\\delta_x = [f(x) - x]/x\\), so \\(f(x) = x(1 + \\delta_x)\\)). What the computer actually calculates is the sum of the floating point representations \\(f (x) + f (y)\\), which may not have an exact floating point representation, in which case it is rounded to the nearest representable number \\(f(f(x) + f(y))\\). Let \\(\\delta_s\\) be the relative error of the \\(f(f(x) + f(y))\\). Then, \\[\\begin{align*} &amp;\\,\\, |f(f(x)+f(y)) - x+y)| \\\\ &amp;= |f(f(x)+f(y)) - f(x) - f(y) + f(x)-x+f(y)-y| \\\\ &amp;\\le |\\delta_s[f(x) + f(y)]| + |\\delta_x x| + |\\delta_y y|\\\\ &amp;\\le |x+y|(\\epsilon_m + \\epsilon^2_m) + |x+y| \\epsilon_m \\\\ &amp;\\approx 2\\epsilon_m |x+y|, \\end{align*}\\] where the higher order terms in \\(\\epsilon_m\\) have been dropped, since they are usually negligible. Thus \\(2\\epsilon_m\\) is an (approximate) bound on the relative error in a single addition of two numbers with the same sign. Example 1.1 (Computing sample sum) (Section 1.5.1 of Srivistava (2009)) Let \\(x_1, \\ldots, x_n\\) be the observations from a random of size \\(n\\). The simplest algorithm to compute the sample sum is to start from \\(s_0 = 0\\) and add one term each time \\(s_j = s_{j-1} + x_j\\). Each addition may result in a number that is not a floating number, with an error denoted by multiple \\((1 + \\epsilon)\\) to that number. For example, when \\(n = 3\\), suppose that \\((x_1, x_2, x_3)\\) are floating numbers. We have: \\[\\begin{align*} f(s_1) &amp;= (0 + x_1)(1 + \\epsilon_1)\\\\ f(s_2) &amp;= (f(s_1) + x_2)(1 + \\epsilon_2)\\\\ f(s_3) &amp;= (f(s_2) + x_3)(1 + \\epsilon_3) \\end{align*}\\] It can be seen that \\[\\begin{align*} f(s_3) = (x_1 + x_2 + x_3) + x_1 (\\epsilon_1 + \\epsilon_2 + \\epsilon_3) + x_2 (\\epsilon_2 + \\epsilon_3) + x_3(\\epsilon_3) + o(\\epsilon_m). \\end{align*}\\] The cumulative error is approximately \\[\\begin{align*} &amp;\\phantom{=} \\| f(s_3) - (x_1 + x_2 + x_3)\\| \\\\ &amp;\\sim \\| x_1 (\\epsilon_1 + \\epsilon_2 + \\epsilon_3) + x_2 (\\epsilon_2 + \\epsilon_3) + x_3(\\epsilon_3) \\|\\\\ &amp;\\le \\|x_1\\| (\\|\\epsilon_1\\| + \\|\\epsilon_2\\| + \\|\\epsilon_3\\|) + \\|x_2\\| (\\|\\epsilon_2\\| + \\|\\epsilon_3\\|) + \\|x_3\\| \\|\\epsilon_3\\| \\\\ &amp;\\le (3 \\|x_1\\| + 2\\|x_2\\| + \\|x_3\\|) \\epsilon_m. \\end{align*}\\] This suggests that better accuracy can be achieved at the expense of sorting in increasing order before computing the sum. Todo: design a decimal system with limited precision to demonstrate the error analysis Subtracting one number from another number of similar magnitude can lead to large relative error. This is more easily illustrated using a decimal system (\\(\\beta = 10\\)); see Section~1.4 of Srivastava (2009). For simplicity, consider the case with \\(t = 4\\). Let \\(x = 122.9572\\) and \\(y = 123.1498\\). Their floating point representations are (1230, 3) and (1231, 3). The resulting difference is \\(-0.1\\) while the actual answer is \\(-0.1926\\). The idea is the same on a binary system. Consider an extreme case: \\[\\begin{align*} f(x) &amp;= 1 / 2 + \\sum_{i=2}^{t-1} x_i / 2^i + 1 / 2^t,\\\\ f(y) &amp;= 1 / 2 + \\sum_{i=2}^{t-1} x_i / 2^i + 0 / 2^t. \\end{align*}\\] The absolute errors of \\(f(x)\\) and \\(f(y)\\) are in \\((0, 1/2^{t+1})\\). The true difference \\(x - y\\) could be anywhere in \\((0, 1 / 2^{t-1})\\) The computed subtraction is \\(f(x) - f(y) = 1 / 2^t\\). So the relative error can be arbitrarily large. The error from subtraction operation also explains why approximating \\(e^{x}\\) by partial sums does not work well for negative \\(x\\) with a large magnitude. Consider an implementation of the exponential function with the Taylor series \\(\\exp(x) = \\sum_{i=0}^{\\infty} x^i / i!\\). fexp &lt;- function(x) { i &lt;- 0 expx &lt;- 1 u &lt;- 1 while (abs(u) &gt; 1.e-8 * abs(expx)) { i &lt;- i + 1 u &lt;- u * x / i expx &lt;- expx + u } expx } options(digits = 10) x &lt;- c(10, 20, 30) cbind(exp( x), sapply( x, fexp)) ## [,1] [,2] ## [1,] 2.202646579e+04 2.202646575e+04 ## [2,] 4.851651954e+08 4.851651931e+08 ## [3,] 1.068647458e+13 1.068647454e+13 cbind(exp(-x), sapply(-x, fexp)) ## [,1] [,2] ## [1,] 4.539992976e-05 4.539992956e-05 ## [2,] 2.061153622e-09 5.621884467e-09 ## [3,] 9.357622969e-14 -3.066812359e-05 The accuracy is poor for \\(x &lt; 0\\). The problem in accuracy occurs because the terms alternate in sign, and some of the terms are much greater than the final answer. It can be fixed by noting that \\(\\exp(-x) = 1 / \\exp(x)\\). fexp2 &lt;- function(x) { if (x &gt;= 0) fexp(x) else 1 / fexp(-x) } cbind(exp(-x), sapply(-x, fexp2)) ## [,1] [,2] ## [1,] 4.539992976e-05 4.539992986e-05 ## [2,] 2.061153622e-09 2.061153632e-09 ## [3,] 9.357622969e-14 9.357623008e-14 1.2.4 Condition Number (Wiki) In numerical analysis, the condition number of a function with respect to an argument measures how much the output value of the function can change for a small change in the input argument. This is used to measure how sensitive a function is to changes or errors in the input, and how much error in the output results from an error in the input. The condition number is an application of the derivative, and is formally defined as the value of the asymptotic worst-case relative change in output for a relative change in input. The function'' is the solution of a problem and thearguments’’ are the data in the problem. The condition number is frequently applied to questions in linear algebra, in which case the derivative is straightforward but the error could be in many different directions, and is thus computed from the geometry of the matrix. More generally, condition numbers can be defined for non-linear functions in several variables. A problem with a low condition number is said to be well-conditioned, while a problem with a high condition number is said to be ill-conditioned. The condition number is a property of the problem. Paired with the problem are any number of algorithms that can be used to solve the problem, that is, to calculate the solution. Some algorithms have a property called backward stability. In general, a backward stable algorithm can be expected to accurately solve well-conditioned problems. Numerical analysis textbooks give formulas for the condition numbers of problems and identify the backward stable algorithms. In the exponential function example, the first implementation is unstable for large negative \\(x\\) while the second is stable. It is important in likelihood maximization to work on the log scale. Use argument log in dpq functions instead of taking logs. This can be important in probalistic networks and MC(MC) where \\(P = P_1 · P_2 \\cdots · P_n\\) quickly underflows to zero. It is also important to use log1p and expm1 alike when caculating \\(\\log(1 + x)\\) and \\(\\exp(x)- 1\\) when \\(|x| \\ll 1\\). One example is the evaluation of distribution and density function of the generalized extreme value (GEV) distribution. The GEV distribution has distribution function \\[\\begin{equation*} F(x; \\mu, \\sigma, \\xi) = \\begin{cases} \\exp[- \\left[ 1 + \\left(\\frac{x - \\mu}{\\sigma}\\right) \\xi\\right]^{-1 / \\xi} &amp; \\xi \\ne 0, \\quad (1 + \\xi (x - \\mu) / \\sigma) &gt; 0,\\\\ \\exp[e^{ -(x - \\mu) / \\sigma}] &amp; \\xi = 0, \\end{cases} \\end{equation*}\\] for \\(\\mu \\in R\\), \\(\\sigma &gt; 0\\), \\(\\xi \\in R\\). curve(evir::pgev(1, xi = x), 1e-20, .01, log=&quot;x&quot;, n = 1025) curve(texmex::pgev(1, 0, 1, x), 1e-20, .01, log=&quot;x&quot;, n = 1025) Figure 1.1: Two implementations of GEV distribution function when \\(\\xi\\) is close to zero. Figure 1.1 shows a comparison of evaluating the GEV distribution function at \\(x = 1\\) for \\(\\mu = 0\\), \\(\\sigma = 1\\), and a sequence of \\(\\xi\\) values near zero using two implementations. 1.3 Exercises Use git to clone the source of Hadley Wickham’s Advanced R Programming from his GitHub repository to a local space on your own computer. Build the book using RStudio. During the building process, you may see error messages due to missing tools on your computer. Read the error messages carefully and fix them, until you get the book built. You may need to install some R packages, some fonts, some latex packages, and some building tools for R packages. On Windows, some codes for parallel computing may not work and need to be swapped out. Document the problems you encountered and how you solved them in an R Markdown file named README.Rmd. Push it to your homework GitHub repository so that it can help other students to build the book. Use bookdown or rmarkdown to produce a report for the following task. Consider approximation of the distribution function of \\(N(0, 1)\\), \\[\\begin{equation} \\Phi(t) = \\int_{-\\infty}^t \\frac{1}{\\sqrt{2\\pi}} e^{-y^2 / 2} dy, \\tag{1.1} \\end{equation}\\] by the Monte Carlo methods: \\[\\begin{equation} \\hat\\Phi(t) = \\frac{1}{n} \\sum_{i=1}^n I(X_i \\le t), \\end{equation}\\] where \\(X_i\\)’s are iid \\(N(0, 1)\\) variables. Experiment with the approximation at \\(n \\in \\{10^2, 10^3, 10^4\\}\\) at \\(t \\in \\{0.0, 0.67, 0.84, 1.28, 1.65, 2.32, 2.58, 3.09, 3.72\\}\\) to form a table. The table should include the true value for comparison. Further, repeat the experiment 100 times. Draw box plots of the 100 approximation errors at each \\(t\\) using ggplot2 (Wickham et al. 2018) for each \\(n\\). The report should look like a manuscript, with a title, an abstract, and multiple sections. It should contain at least one math equation, one table, one figure, and one chunk of R code. The template of our Data Science Lab can be helpful: https://statds.org/template/, the source of which is at https://github.com/statds/dslab-templates. Explain how .Machine$double.xmax, .Machine$double.xmin, .Machine$double.eps, and .Machine@double.neg.eps are defined using the 64-bit double precision floating point arithmetic. 1.4 Course Project Each student is required to work independently on a class project on a topic of your choice that involves computing. Examples are an investigation of the properties of a methodology you find interesting, a comparison of several methods on a variety of problems, a comprehensive analysis of a real data with methodologies from the course, or an R package. The project should represent new work, not something you have done for another course or as part of your thesis. I have a collection of small problems that might be suitable. Projects from Kaggle data science competitions may be good choices too. Project proposal is due in week 7, the middle of the semester. This is a detailed, one-page description of what you plan to do, including question(s) to be addressed and models/methods to be used. Project interim report is due in week 12. This informal report will indicate that your project is ``on track’’. The report includes results obtained thus far and a brief summary of what they mean and what remains to be done. Students often start working on this too late. Your do not want to make the same mistake. Final project report is due in the exam week. The final form of the report should use the Data Science Lab template, of no more than 6 pages (single spaced, font no smaller than 11pt, 1 inch margin, with references, graphics and tables included). With more effort, a good project can turn into a paper and be submitted to ENAR student paper competition (Oct. 15) and ASA student paper competition (Dec. 15). Project ideas: An animated demonstration of different methods in optimx with data examples. References "],
["nla.html", "Chapter 2 Numerical Linear Algebra", " Chapter 2 Numerical Linear Algebra This chapter should cover basics of vector/matrix operation; matrix factorization, and their applications in statisticcs. "],
["optim.html", "Chapter 3 Optimization 3.1 Univariate Optimizations 3.2 Multivariate Optimization 3.3 Exercises", " Chapter 3 Optimization Recall the MLE example in Chapter 1. Consider a random sample \\(X_1, \\ldots, X_n\\) of size \\(n\\) coming from a univariate distribution with density function \\(f(x | \\theta)\\), where \\(\\theta\\) is a parameter vector. The MLE \\(\\hat\\theta_n\\) of the unknown parameter \\(\\theta\\) is obtained by maximizing the loglikelihood function \\[ l(\\theta) = \\sum_{i=1}^n \\log f(X_i | \\theta) \\] with respect to \\(\\theta\\). Typically, \\(\\hat\\theta\\) is obtained by solving the score equation \\[ l&#39;(\\theta) = \\sum_{i=1}^n \\frac{\\partial}{\\partial \\theta} \\log f(X_i; \\theta) = 0, \\] i.e., the derivative of the loglikelihood function equated to zero. From mathematical statistics, it is known that, under certain regularity conditions, \\[\\begin{align*} \\mathbb{E}\\frac{\\partial}{\\partial \\theta} \\log f(X; \\theta) &amp;= 0,\\\\ \\mathbb{E}\\frac{\\partial}{\\partial \\theta} \\log f(X; \\theta) \\left[\\frac{\\partial}{\\partial \\theta}\\ log f(X; \\theta) \\right]^{\\top} &amp;= - \\mathbb{E} \\frac{\\partial^2}{\\partial \\theta \\partial \\theta^{\\top}} \\log f(X; \\theta). \\end{align*}\\] The expectation in the second equation is known as the Fisher information \\(I(\\theta)\\), a nonnegative definite matrix. Large sample results state that, as \\(n \\to \\infty\\), \\(\\hat\\theta_n\\) is consistent for \\(\\theta\\) and \\(\\sqrt{n} (\\hat\\theta_n - \\theta)\\) converges in distribution to \\(N(0, I^{-1}(\\theta))\\). The asymptotic variance of \\(\\hat\\theta\\) can be estimated by inverting the observed Fisher information matrix \\(l&#39;&#39;(\\hat\\theta_n)\\). More generally in Statistics, M-estimators are a broad class of extremum estimators obtained by maximizing an data dependent objective function. Both non-linear least squares and maximum likelihood estimation are special cases of M-estimators. The definition of M-estimators was motivated by robust statistics, which contributed new types of M-estimators. When the objective function is smooth, the M-estimator can be obtained by solving the corresponding “score” equation. Clearly, optimization or root-finding are very important in Statistics. 3.1 Univariate Optimizations Optimization and root finding are closely related. Consider maximization of a smooth and differentiable function \\(g(x)\\). A necessary condition at the maximum is \\(f(x) = g&#39;(x) = 0\\). Univariate illustrations help to gain insights about the ideas. 3.1.1 Bisection Method The bisection method repeatedly bisects an interval and then selects a subinterval in which a root must lie for further processing. It is a very simple and robust method, but relatively slow. The method is applicable for solving the equation \\(f(x) = 0\\) for the real variable \\(x\\), where \\(f\\) is a continuous function defined on an interval \\([a, b]\\) and \\(f(a)\\) and \\(f(b)\\) have opposite signs. This method is illustrated with an animation using the package animation (Xie 2017). animation::bisection.method() 3.1.2 Newton’s Method A fast approach to find roots of a differentiable function \\(f(x)\\). The methods starts from some initial value \\(x_0\\), and for \\(t = 0, 1, \\ldots\\), compute \\[\\begin{align*} x_{t+1} = x_t - \\frac{f(x_t)}{f&#39;(x_t)} \\end{align*}\\] until \\(x_t\\) converges. The method is based on a linear expansion of \\(f(x)\\). The method is also known as Newton–Raphson iteration. It needs an initial value \\(x_0\\). If \\(f(x) = 0\\) has multiple solutions, the end result depends on \\(x_0\\). Applied to optimization of \\(g\\), this method requires the Hessian \\(g&#39;&#39;\\), which can be difficult to obtain, especially for multivariate functions. Many variants of Newton’s method avoid the computation of the Hessian. For example, to obtain MLE with likelihood \\(l(\\theta)\\), Fisher scoring replaces \\(-l&#39;&#39;(\\theta_t)\\) with \\(I(\\theta_t)\\). Generally, one uses Fisher scoring in the beginning to make rapid improvements, and Newton’s method for refinement near the end. The secant method approximates \\(f&#39;(x_t)\\) by \\[\\begin{align*} \\frac{f(x_t) - f(x_{t-1})}{x_t - x_{t-1}}. \\end{align*}\\] Given initial values \\(x_0\\) and \\(x_1\\), the iteration is \\[\\begin{align*} x_{t+1} = x_t - \\frac{f(x_t)(x_t- x_{t-1})}{f(x_t) - f(x_{t-1})}. \\end{align*}\\] 3.1.3 Fixed Point Iteration A fixed point of a function is a point whose evaluation by that function equals to itself, i.e., \\(x=G(x)\\). Fixed point iteration: the natural way of hunting for a fixed point is to use \\(x_{t+1}=G(x_t)\\). Definition 3.1 A function \\(G\\) is contractive on \\([a,b]\\) if \\[\\begin{align*} (1).\\,\\, &amp; G(x)\\in [a,b] \\mbox{ whenever } x\\in [a,b],\\\\ (2).\\,\\, &amp; |G(x_1) - G(x_2)| \\leq \\lambda |x_1-x_2| \\mbox{ for all } x_1,x_2\\in [a,b] \\mbox{ and some } \\lambda\\in [0,1). \\end{align*}\\] Theorem 3.1 If \\(G\\) is contractive on \\([a,b]\\), then there is a unique fixed point \\(x^*\\in [a,b]\\), and the fixed point iteration convergence to it when starting inside the interval. Convergence: \\(|x_{t+1}-x_{t}|=|G(x_t)-G(x_{t-1})|\\leq \\lambda |x_t - x_{t-1}|\\leq \\lambda^{t}|x_1-x_0| \\rightarrow 0\\), as \\(t\\rightarrow \\infty\\). It follows that \\(\\{x_t\\}\\) convergent to a limit \\(x^*\\). Application in root finding: for solving \\(f(x)=0\\), we can simply let \\(G(x) = x + \\alpha f(x)\\), where \\(\\alpha \\neq 0\\) is a constant. Required Lipschitz condition: \\(|x - y + \\alpha [f(x) - f(y)]| \\le \\lambda|x-y|\\), for some \\(\\lambda \\in [0,1)\\) and for all \\(x,y\\in [a,b]\\). This holds if \\(|G&#39;(x)|\\leq \\lambda\\) for some \\(\\lambda \\in [0,1)\\) and for all \\(x\\in [a,b]\\), i.e., \\(|1+\\alpha f&#39;(x)|\\leq\\lambda\\). (use mean value theorem.) Newton’s methods: \\(G(x) = x - f(x)/f&#39;(x)\\). So it is as if \\(\\alpha_t\\) is chosen adaptively as \\(\\alpha_t=-1/f&#39;(x_t)\\). This leads to a faster convergence order (quadratic). 3.2 Multivariate Optimization Consider maximizing a real-valued objective function \\(g(x)\\) of \\(p\\)-dimensional vector \\(x\\). 3.2.1 Newton-Raphson Method The Newton-Raphson method is a generalization of univariate Newton’s root finding algorithm to the multivariate case. 3.2.2 Variants of Newton-Raphson Method The motivation is to avoid the calculation of the Hessian matrix in updatings with the form \\[ x^{(t + 1)} = x^{(t)} - (M^{(t)})^{-1} g&#39;(x), \\] where \\(g&#39;\\) is the gradient of \\(g\\) and \\(M\\) is a substitute of \\(g&#39;&#39;(x)\\). 3.2.2.1 Fisher Scoring Choose \\(M\\) as the negative Fisher information matrix. For location-families, it can be shown that the information matrix is free of the location parameter. 3.2.2.2 Steepest Ascent Choose \\(M = \\alpha^{(t)} I_p\\) for \\(\\alpha^{(t)} &gt; 0\\) so the updating in the direction of he steepest ascent. 3.2.2.3 Discrete Newton Approximate the Hessian matrix with finite-difference quotients. 3.2.2.4 Fixed-Point Iteration Choose \\(M^{(t)} = M\\) for all \\(t\\). An example is \\(M = g&#39;&#39;(x_0)\\). 3.2.2.5 Quasi-Newton Rank-one or rank-two update. The celebrated BFGS is a rank-two update method. Box constraints can be allowed with L-BFGS-B. 3.2.2.6 Coordinate Ascent 3.2.2.7 Conjugate Gradient 3.2.2.8 Gauss-Seidel 3.2.2.9 Gauss-Newton 3.2.3 Nelder-Mead (Simplex) Algorithm Not even the gradient is needed. Possible moves are: reflection; expansion; outer contraction; inner contraction; shrinkage. Consider minimizing this simple function. f1 &lt;- function(x) { x1 &lt;- x[1] x2 &lt;- x[2] x1^2 + 3*x2^2 } Apparently the minimizer is \\((0, 0)\\). Let’s track all the points at which the function is evaluated in the Nelder-Mead algorithm. trace(f1, exit = quote(print(c(returnValue(), x)))) ## [1] &quot;f1&quot; optim(c(1, 1), f1, control = list(trace = TRUE)) ## Nelder-Mead direct search function minimizer ## Tracing fn(par, ...) on exit ## [1] 4 1 1 ## function value for initial parameters = 4.000000 ## Scaled convergence tolerance is 5.96046e-08 ## Stepsize computed as 0.100000 ## Tracing fn(par, ...) on exit ## [1] 4.21 1.10 1.00 ## Tracing fn(par, ...) on exit ## [1] 4.63 1.00 1.10 ## BUILD 3 4.630000 4.000000 ## Tracing fn(par, ...) on exit ## [1] 3.64 1.10 0.90 ## Tracing fn(par, ...) on exit ## [1] 3.2425 1.1500 0.8000 ## EXTENSION 5 4.210000 3.242500 ## Tracing fn(par, ...) on exit ## [1] 3.0225 1.0500 0.8000 ## Tracing fn(par, ...) on exit ## [1] 2.520625 1.025000 0.700000 ## EXTENSION 7 4.000000 2.520625 ## Tracing fn(par, ...) on exit ## [1] 2.130625 1.175000 0.500000 ## Tracing fn(par, ...) on exit ## [1] 1.78140625 1.26250000 0.25000000 ## EXTENSION 9 3.242500 1.781406 ## Tracing fn(par, ...) on exit ## [1] 1.36140625 1.13750000 0.15000000 ## Tracing fn(par, ...) on exit ## [1] 1.371601563 1.131250000 -0.175000000 ## REFLECTION 11 2.520625 1.361406 ## Tracing fn(par, ...) on exit ## [1] 2.160625 1.375000 -0.300000 ## Tracing fn(par, ...) on exit ## [1] 1.66515625 1.28750000 -0.05000000 ## LO-REDUCTION 13 1.781406 1.361406 ## Tracing fn(par, ...) on exit ## [1] 1.41890625 1.16250000 -0.15000000 ## Tracing fn(par, ...) on exit ## [1] 1.41765625 1.18750000 -0.05000000 ## LO-REDUCTION 15 1.665156 1.361406 ## Tracing fn(par, ...) on exit ## [1] 1.14390625 1.03750000 0.15000000 ## Tracing fn(par, ...) on exit ## [1] 1.02015625 0.91250000 0.25000000 ## EXTENSION 17 1.417656 1.020156 ## Tracing fn(par, ...) on exit ## [1] 1.35140625 0.86250000 0.45000000 ## Tracing fn(par, ...) on exit ## [1] 1.207539063 0.943750000 0.325000000 ## LO-REDUCTION 19 1.361406 1.020156 ## Tracing fn(par, ...) on exit ## [1] 1.058476563 0.718750000 0.425000000 ## Tracing fn(par, ...) on exit ## [1] 1.058791504 0.823437500 0.356250000 ## LO-REDUCTION 21 1.207539 1.020156 ## Tracing fn(par, ...) on exit ## [1] 0.84015625 0.68750000 0.35000000 ## Tracing fn(par, ...) on exit ## [1] 0.7071191406 0.5593750000 0.3625000000 ## EXTENSION 23 1.058477 0.707119 ## Tracing fn(par, ...) on exit ## [1] 0.6726660156 0.7531250000 0.1875000000 ## Tracing fn(par, ...) on exit ## [1] 0.6075610352 0.7703125000 0.0687500000 ## EXTENSION 25 1.020156 0.607561 ## Tracing fn(par, ...) on exit ## [1] 0.2726000977 0.4171875000 0.1812500000 ## Tracing fn(par, ...) on exit ## [1] 0.0934576416 0.1695312500 0.1468750000 ## EXTENSION 27 0.707119 0.093458 ## Tracing fn(par, ...) on exit ## [1] 0.2094732666 0.3804687500 -0.1468750000 ## Tracing fn(par, ...) on exit ## [1] 0.1819354630 0.4251953125 -0.0195312500 ## LO-REDUCTION 29 0.607561 0.093458 ## Tracing fn(par, ...) on exit ## [1] 0.04113010406 -0.17558593750 0.05859375000 ## Tracing fn(par, ...) on exit ## [1] 0.4291896152 -0.6485351562 0.0535156250 ## REFLECTION 31 0.181935 0.041130 ## Tracing fn(par, ...) on exit ## [1] 0.3378515625 -0.4312500000 0.2250000000 ## Tracing fn(par, ...) on exit ## [1] 0.04974851847 0.21108398438 0.04160156250 ## HI-REDUCTION 33 0.093458 0.041130 ## Tracing fn(par, ...) on exit ## [1] 0.02450187922 -0.13403320312 -0.04667968750 ## Tracing fn(par, ...) on exit ## [1] 0.1434302193 -0.2858154297 -0.1434570313 ## REFLECTION 35 0.049749 0.024502 ## Tracing fn(par, ...) on exit ## [1] 0.2737757874 -0.5207031250 -0.0296875000 ## Tracing fn(par, ...) on exit ## [1] 0.002488067299 0.028137207031 0.023779296875 ## HI-REDUCTION 37 0.041130 0.002488 ## Tracing fn(par, ...) on exit ## [1] 0.02478057280 0.06968994141 -0.08149414063 ## Tracing fn(par, ...) on exit ## [1] 0.006549060354 0.008370971680 -0.046472167969 ## LO-REDUCTION 39 0.024502 0.002488 ## Tracing fn(par, ...) on exit ## [1] 0.03081046500 0.17054138184 0.02398681641 ## Tracing fn(par, ...) on exit ## [1] 0.005876474013 -0.057889556885 -0.029013061523 ## HI-REDUCTION 41 0.006549 0.002488 ## Tracing fn(par, ...) on exit ## [1] 0.00655520537 -0.03812332153 0.04123840332 ## Tracing fn(par, ...) on exit ## [1] 0.001817880561 -0.003252601624 -0.024544525146 ## HI-REDUCTION 43 0.005876 0.001818 ## Tracing fn(par, ...) on exit ## [1] 0.009245382194 0.082774162292 0.028247833252 ## Tracing fn(par, ...) on exit ## [1] 0.001164442539 -0.022723627090 -0.014697837830 ## HI-REDUCTION 45 0.002488 0.001164 ## Tracing fn(par, ...) on exit ## [1] 0.01484345276 -0.05411343575 -0.06302165985 ## Tracing fn(par, ...) on exit ## [1] 7.034119489e-05 7.574546337e-03 2.079057693e-03 ## HI-REDUCTION 47 0.001818 0.000070 ## Tracing fn(par, ...) on exit ## [1] 0.0005681963978 -0.0118964791298 0.0119257450104 ## Tracing fn(par, ...) on exit ## [1] 0.0001184377323 -0.0097355097532 0.0028081774712 ## LO-REDUCTION 49 0.001164 0.000070 ## Tracing fn(par, ...) on exit ## [1] 0.00157354839 0.02056266367 0.01958507299 ## Tracing fn(par, ...) on exit ## [1] 0.0002542833343 -0.0119020543992 -0.0061271101236 ## HI-REDUCTION 51 0.000254 0.000070 ## Tracing fn(par, ...) on exit ## [1] 0.0004588362599 0.0097410909832 0.0110143452883 ## Tracing fn(par, ...) on exit ## [1] 5.231264892e-05 -6.491268054e-03 -1.841746271e-03 ## HI-REDUCTION 53 0.000118 0.000052 ## Tracing fn(par, ...) on exit ## [1] 0.0001368742313 0.0108187880367 -0.0025708660483 ## Tracing fn(par, ...) on exit ## [1] 2.755657856e-05 -4.596935306e-03 1.463416591e-03 ## HI-REDUCTION 55 0.000070 0.000028 ## Tracing fn(par, ...) on exit ## [1] 0.0003664144843 -0.0186627496965 -0.0024573873729 ## Tracing fn(par, ...) on exit ## [1] 3.709447626e-06 1.015222329e-03 9.449464269e-04 ## HI-REDUCTION 57 0.000052 0.000004 ## Tracing fn(par, ...) on exit ## [1] 6.265579765e-05 2.909555077e-03 4.250109289e-03 ## Tracing fn(par, ...) on exit ## [1] 1.745326335e-05 -4.141062271e-03 -3.187823808e-04 ## HI-REDUCTION 59 0.000028 0.000004 ## Tracing fn(par, ...) on exit ## [1] 4.267097041e-06 1.471095363e-03 -8.372525452e-04 ## Tracing fn(par, ...) on exit ## [1] 2.081739918e-07 -4.591230390e-05 -2.620852611e-04 ## LO-REDUCTION 61 0.000017 0.000000 ## Tracing fn(par, ...) on exit ## [1] 2.912577439e-05 5.110372296e-03 1.001643547e-03 ## Tracing fn(par, ...) on exit ## [1] 3.342713216e-06 -1.828203629e-03 1.132410107e-05 ## HI-REDUCTION 63 0.000004 0.000000 ## Tracing fn(par, ...) on exit ## [1] 1.263742549e-05 -2.889338262e-03 -1.195707587e-03 ## Tracing fn(par, ...) on exit ## [1] 5.052935499e-07 3.908218105e-05 4.097829235e-04 ## HI-REDUCTION 65 0.000003 0.000000 ## Tracing fn(par, ...) on exit ## [1] 3.373194695e-06 1.821373506e-03 1.363735613e-04 ## Tracing fn(par, ...) on exit ## [1] 8.441475784e-07 -9.158093454e-04 4.258646613e-05 ## HI-REDUCTION 67 0.000001 0.000000 ## Tracing fn(par, ...) on exit ## [1] 8.593883177e-07 9.089792225e-04 1.051111963e-04 ## Tracing fn(par, ...) on exit ## [1] 2.214112614e-07 -4.596122034e-04 5.821764866e-05 ## HI-REDUCTION 69 0.000001 0.000000 ## Tracing fn(par, ...) on exit ## [1] 1.426297385e-06 -5.446066884e-04 -6.136505358e-04 ## Tracing fn(par, ...) on exit ## [1] 8.249310260e-08 -1.068400363e-04 1.539245586e-04 ## HI-REDUCTION 71 0.000000 0.000000 ## Tracing fn(par, ...) on exit ## [1] 1.772082428e-07 3.068598632e-04 -1.663783511e-04 ## Tracing fn(par, ...) on exit ## [1] 4.973221276e-08 1.152418466e-04 -1.102293512e-04 ## LO-REDUCTION 73 0.000000 0.000000 ## Tracing fn(par, ...) on exit ## [1] 2.834551078e-07 5.431411415e-05 3.057804685e-04 ## Tracing fn(par, ...) on exit ## [1] 4.372055919e-08 -2.085569939e-05 -1.201188287e-04 ## Exiting from Nelder Mead minimizer ## 75 function evaluations used ## $par ## [1] -2.085569939e-05 -1.201188287e-04 ## ## $value ## [1] 4.372055919e-08 ## ## $counts ## function gradient ## 75 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL untrace(f1) 3.2.4 Optimization with R See conversation with John Nash about r optim and r optimx. 3.3 Exercises 3.3.1 Cauchy with unknown location. Consider estimating the location parameter of a Cauchy distribution with a known scale parameter. The density function is \\[\\begin{align*} f(x; \\theta) = \\frac{1}{\\pi[1 + (x - \\theta)^2]}, \\quad x \\in R, \\quad \\theta \\in R. \\end{align*}\\] Let \\(X_1, \\ldots, X_n\\) be a random sample of size \\(n\\) and \\(\\ell(\\theta)\\) the log-likelihood function of \\(\\theta\\) based on the sample. - Show that \\[\\begin{align*} \\ell(\\theta) &amp;= -n\\ln \\pi - \\sum_{i=1}^n \\ln [1+(\\theta-X_i)^2], \\\\ \\ell&#39;(\\theta) &amp;= -2 \\sum_{i=1}^n \\frac{\\theta-X_i}{1+(\\theta-X_i)^2}, \\\\ \\ell&#39;&#39;(\\theta) &amp;= -2 \\sum_{i=1}^n \\frac{1-(\\theta-X_i)^2}{[1+(\\theta-X_i)^2]^2}, \\\\ I_n(\\theta) &amp;= \\frac{4n}{\\pi} \\int_{-\\infty}^\\infty \\frac{x^2\\,\\mathrm{d}x}{(1+x^2)^3} = n/2, \\end{align*}\\] where \\(I_n\\) is the Fisher information of this sample. - Set the random seed as \\(20180909\\) and generate a random sample of size \\(n = 10\\) with \\(\\theta = 5\\). Implement a loglikelihood function and plot against \\(\\theta\\). - Find the MLE of \\(\\theta\\) using the Newton–Raphson method with initial values on a grid starting from \\(-10\\) to \\(30\\) with increment \\(0.5\\). Summarize the results. - Improved the Newton–Raphson method by halving the steps if the likelihood is not improved. - Apply fixed-point iterations using \\(G(\\theta)=\\alpha \\ell&#39;(\\theta) + \\theta\\), with scaling choices of \\(\\alpha \\in \\{1, 0.64, 0.25\\}\\) and the same initial values as above. - First use Fisher scoring to find the MLE for \\(\\theta\\), then refine the estimate by running Newton-Raphson method. Try the same starting points as above. - Comment on the results from different methods (speed, stability, etc.). 3.3.2 Many local maxima Consider the probability density function with parameter \\(\\theta\\): \\[\\begin{align*} f(x; \\theta) = \\frac{1 - \\cos(x - \\theta)}{2\\pi}, \\quad 0\\le x\\le 2\\pi, \\quad \\theta \\in (-\\pi, \\pi). \\end{align*}\\] A random sample from the distribution is x &lt;- c(3.91, 4.85, 2.28, 4.06, 3.70, 4.04, 5.46, 3.53, 2.28, 1.96, 2.53, 3.88, 2.22, 3.47, 4.82, 2.46, 2.99, 2.54, 0.52) Find the the log-likelihood function of \\(\\theta\\) based on the sample and plot it between \\(-\\pi\\) and \\(\\pi\\). Find the method-of-moments estimator of \\(\\theta\\). That is, the estimator \\(\\tilde\\theta_n\\) is value of \\(\\theta\\) with \\[\\begin{align*} \\mathbb{E}(X \\mid \\theta) = \\bar X_n, \\end{align*}\\] where \\(\\bar X_n\\) is the sample mean. This means you have to first find the expression for \\(\\mathbb{E}(X \\mid \\theta)\\). Find the MLE for \\(\\theta\\) using the Newton–Raphson method initial value \\(\\theta_0 = \\tilde\\theta_n\\). What solutions do you find when you start at \\(\\theta_0 = -2.7\\) and \\(\\theta_0 = 2.7\\)? Repeat the above using 200 equally spaced starting values between \\(-\\pi\\) and \\(\\pi\\). Partition the values into sets of attraction. That is, divide the set of starting values into separate groups, with each group corresponding to a separate unique outcome of the optimization. 3.3.3 Modeling beetle data The counts of a floor beetle at various time points (in days) are given in a dataset. beetles &lt;- data.frame( days = c(0, 8, 28, 41, 63, 69, 97, 117, 135, 154), beetles = c(2, 47, 192, 256, 768, 896, 1120, 896, 1184, 1024)) A simple model for population growth is the logistic model given by \\[ \\frac{\\mathrm{d}N}{\\mathrm{d}t} = r N(1 - \\frac{N}{K}), \\] where \\(N\\) is the population size, \\(t\\) is time, \\(r\\) is an unknown growth rate parameter, and \\(K\\) is an unknown parameter that represents the population carrying capacity of the environment. The solution to the differential equation is given by \\[ N_t = f(t) = \\frac{K N_0}{N_0 + (K - N_0)\\exp(-rt)}, \\] where \\(N_t\\) denotes the population size at time \\(t\\). - Fit the population growth model to the beetles data using the Gauss-Newton approach, to minimize the sum of squared errors between model predictions and observed counts. - Show the contour plot of the sum of squared errors. - In many population modeling application, an assumption of lognormality is adopted. That is , we assume that \\(\\log N_t\\) are independent and normally distributed with mean \\(\\log f(t)\\) and variance \\(\\sigma^2\\). Find the maximum likelihood estimators of \\(\\theta = (r, K, \\sigma^2)\\) using any suitable method of your choice. Estimate the variance your parameter estimates. References "],
["em-algorithm.html", "Chapter 4 EM Algorithm", " Chapter 4 EM Algorithm EM has its own chapter for its importance in statistics. "],
["random-number-generation.html", "Chapter 5 Random Number Generation", " Chapter 5 Random Number Generation Simulation basics. "],
["markov-chain-monte-carlo.html", "Chapter 6 Markov Chain Monte Carlo", " Chapter 6 Markov Chain Monte Carlo Someone may need this for the course project. "],
["references.html", "References", " References "]
]
